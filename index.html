
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-158231962-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-158231962-1');
    </script>
    <!-- <p style="text-align:right"><a href="index_spanish.html">Español</a></p> -->
  <title>Maria Bauza Villalonga</title>
    
  <meta name="author" content="Maria Bauza Villalonga">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/mbv-icon-v2.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:right"><a href="index_spanish.html">Español</a></p>
              <p style="text-align:center">
                <name>Maria Bauza Villalonga</name>
              </p>
              <p>I am research scientist at Google DeepMind working on robotic manipulation. I earned my PhD at MIT working with Prof. Alberto Rodriguez.
              </p>
              <p>
                I develop algorithms and solutions that enable robots to solve new tasks with high accuracy and dexterity. 
                My research was supported by <a href="https://fundacionlacaixa.org/en/postgraduate-fellowships-abroad-call" target="_blank"> 
                    LaCaixa </a> and <a href="https://research.fb.com/fellows/villalonga-maria-bauza/" target="_blank"> Facebook </a> fellowships.
              </p>
              <p style="text-align:center">
                <a href="mailto:{first_name}0bauza@gmail.com">{first_name}0bauza@gmail.com</a> &nbsp/&nbsp
                <a href="data/CV.pdf"  target="_blank">CV</a> &nbsp/&nbsp
                <a href="data/bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=gkC58XkAAAAJ&hl" target="_blank">Google Scholar</a> &nbsp/&nbsp                 <a href="https://www.linkedin.com/in/mariabauza/" target="_blank">LinkedIn</a> &nbsp &nbsp
                <!-- <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:35%;max-width:40%">
              <a href="images/www/MariaBauza2025.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/www/MariaBauza2025.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px;"><tbody>
            <tr>
            <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p align="justify">
                 <a href="https://www.youtube.com/watch?v=T7ZO_cxgHlo" target="_blank"> My research </a> focuses on developing algorithms for precise robotic generalization: 
                making robots capable of solving many tasks without compromising their performance and reliability. 
                By learning general AI models of perception and control, 
                we can provide robots with the right tools to thrive in diverse environments and task requirements.
                <p/> <p align="justify"> 
                In my work, I have studied how learning AI models allows precise control, 
                and how developing accurate visuo-tactile perception enables solving complex tasks,
                such as grasping, localization, and precise placing without prior experience.  
                My goal is to continue developing algorithms that make robots dexterous and versatile at manipulating their environment.
                <!--
                I develop algorithms that enable embodied intelligence to make- robots perceive and
                interact with their environment accurately. In my work, I have studied both the power of applying
                the most recent advances in AI and computer vision to control robotic systems and the
                capabilities of high-resolution tactile sensing to solve complex tasks, such as grasping,
                localization, and precise placing. My goal is to effectively leverage AI on any source of sensing
                to make robots more accurate, reactive, and dexterous. -->
              </p>
              <p align="justify">
              <!-- <strong>  Final PhD work: precise pick-and-placing of objects without prior experience! </strong> Why is this important? Currently, industry can't solve
              this problem for a large variety of objects. Our system can, enabling robotics solutions in a wide variety of applications where flexibility is key.
              </p> --> 
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;text-align:center;"><tbody>
            <tr>
            <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <!-- <td style="padding:2.5%;width:40%;max-width:40%"> -->
              
            <!-- <figure>
                <img  src="images/www/demo_5obj.webp"  width="600" class="center">
                <figcaption>The robot is capable to precisely pick-and-place objects that it had never interacted with before.</figcaption>
            </figure> -->

       <figure>
          <!-- Create a flex container to hold the images -->
          <div style="display: flex; justify-content: center; align-items: flex-start; gap: 15px;">
        
             <!-- First Image -->
             <img src="exostart-earbuds.gif" height="300">
        
             <!-- Second Image -->
             <img src="images/www/demo_5obj.webp" height="300">

         </div>
          <figcaption style="text-align: left;">
            ExoStart: RL + sim2real transfer for hand control.  
            <span style="margin-left: 140px;"> Precise pick-and-place of objects using SimPLE.</span>
          </figcaption>
          <!-- <figcaption style="text-align: left> a) ExoStart: RL + sim2real for hand control.    <span style="margin-left: 50px;">    b) Precise pick-and-place of objects using SimPLE. </span></figcaption> -->
        </figure>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Latest News</heading>
                <p>
                <strong> June 2025 </strong> Invited talk at A Robot Touch of AI: London Summer School in Robotics & AI 2025.
                </p><p>
                <strong> May 2025 </strong> Tech Talk at ICRA Stage Presentations.
                </p><p>
                <strong> May 2025 </strong> Invited talk at <a href="https://drive.google.com/file/d/1SBGmzPLKs00b9_JsksfHvT9B05H-wgzm/view" target="_blank"> ICRA Workshop Beyond Pick and Place".</a>
                </p><p>
                <strong> May 2025 </strong> Invited talk at Microsoft Cortex AI Research Talk Series.
                </p><p>
                <strong> March 2025 </strong> Invited guest lecture at Oxford Saïd Business School MBA.
                </p><p>
                <strong> November 2024 </strong> Invited talk at CoRL Workshop on Learning Robotic Assembly.
                </p><p>
                <strong> October 2024 </strong> Invited talk at DevFest Menorca.
                </p><p>
                <strong> August 2024 </strong> Invited talk at Forum Illa del Rei on AI.
                </p><p>
                <strong> June 2024 </strong> Invited talk at Oxford Business school Analytics and AI class.
                </p><p>
                <strong> May 2024 </strong> Invited talk at ICRA Workshop Vitac.
                </p><p>
                <strong> April 2024 </strong> Invited talk at ETH lab seminar.
                </p><p>
                <strong> December 2023 </strong> Invited talk at Deep Learning Barcelona Symposium.
                </p><p>
                <strong> December 2023 </strong> Invited talk and panel at Balearic Ecosystem for AI.
                </p><p>
                <strong> July 2023 </strong> Guest lecturer at Oxford’s MBA class on Machine Learning for Business.
                </p><p>
                <strong> January 2023 </strong> Guest interview at UK Robotics and Autonomous Systems Network’s podcast, Robot Talk.
                </p><p>
                <strong> September 2022 </strong> Invited talk at the RSS 2022 workshop on The Science of Bumping Into Things.
                </p><p>
                <strong> August 2022 </strong>   <a href="https://www.youtube.com/watch?v=T7ZO_cxgHlo" target="_blank"> Thesis defense: Visuo-tactile perception for dexterous robotic manipulation (video).</a>
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <heading> Publications </heading>
           </tr>



           <tr bgcolor="#ffffd0"  onmouseout="ExoStart_stop()" onmouseover="ExoStart_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div id='ExoStart_blurry' class='hidden'><img src="exostart.gif"  width="160"></div>
              <div id='ExoStart_sharp'>
                <a href="images/www/exostart.png"><img src="images/www/exostart.png"  width="160"></a>
              </div>
              <script type="text/javascript">
                function ExoStart_start() {
                  document.getElementById('ExoStart_blurry').style.display = 'inline';
                  document.getElementById('ExoStart_sharp').style.display = 'none';
                }

                function ExoStart_stop() {
                  document.getElementById('ExoStart_blurry').style.display = 'none';
                  document.getElementById('ExoStart_sharp').style.display = 'inline';
                }
                ExoStart_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations</papertitle>
              <br>
              Z. Si, J. Chen, E. Karagozler, A. Bronars, J. Hutchinson, T. Lampe, N. Gileadi, T. Howell, S. Saliceti, L. Barczyk, I. Correa, T. Erez, M. Shridhar, M. Martins, K. Bousmalis, N. Heess, F. Nori, <strong>M. Bauza</strong>
              <br>
              <em>submitted to ICRA 2026</em>
              <br>
              <a href="https://arxiv.org/pdf/2506.11775" target="_blank">PDF</a> /
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/ExoStart.bib" target="_blank">bibtex</a> / -->
              <a href="https://sites.google.com/view/exostart" target="_blank">website</a>
              <p></p>
              <p> We present ExoStart, a general and scalable learning framework that leverages the power of human dexterity for robotic hand control. In particular, we obtain high-quality data by collecting direct demonstrations using an low-cost exoeskeleton, capturing the rich behaviors that humans naturally can demonstrate. </p>
            </td>
           </tr>
            <tr onmouseout="Gemini25_stop()" onmouseover="Gemini25_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='Gemini25_blurry' class='hidden'><img src="images/www/gemini25.png"  width="160"></div>
               <div id='Gemini25_sharp'>
                 <a href="images/www/gemini25.png"><img src="images/www/gemini25.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function Gemini25_start() {
                   document.getElementById('Gemini25_blurry').style.display = 'inline';
                   document.getElementById('Gemini25_sharp').style.display = 'none';
                 }

                 function Gemini25_stop() {
                   document.getElementById('Gemini25_blurry').style.display = 'none';
                   document.getElementById('Gemini25_sharp').style.display = 'inline';
                 }
                 Gemini25_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities</papertitle>
              <br>
              Gemini Team
              <br>
              <em>Technical Report</em>
              <br>
              <a href="https://arxiv.org/pdf/2507.06261" target="_blank">PDF</a> /
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/Gemini25.bib" target="_blank">bibtex</a> / -->
              <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/" target="_blank">website</a>
              <p></p>
              <p> Gemini 2.5 Pro is Google DeepMind's most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. Gemini 2.5 Pro is also a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. </p>
            </td>
           </tr>
            <tr onmouseout="GeminiRobotics_stop()" onmouseover="GeminiRobotics_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='GeminiRobotics_blurry' class='hidden'><img src="geminirobotics.gif"  width="160"></div>
               <div id='GeminiRobotics_sharp'>
                 <a href="images/www/geminirobotics.png"><img src="geminirobotics.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function GeminiRobotics_start() {
                   document.getElementById('GeminiRobotics_blurry').style.display = 'inline';
                   document.getElementById('GeminiRobotics_sharp').style.display = 'none';
                 }

                 function GeminiRobotics_stop() {
                   document.getElementById('GeminiRobotics_blurry').style.display = 'none';
                   document.getElementById('GeminiRobotics_sharp').style.display = 'inline';
                 }
                 GeminiRobotics_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Gemini robotics: Bringing ai into the physical world</papertitle>
              <br>
              Gemini Robotics Team
              <br>
              <em>Technical Report</em>
              <br>
              <a href="https://arxiv.org/pdf/2503.20020" target="_blank">PDF</a> /
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/GeminiRobotics.bib" target="_blank">bibtex</a> / -->
              <a href="https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/" target="_blank">website</a>
              <p></p>
              <p> Gemini Robotics is an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks, handling unseen environments as well as following diverse, open vocabulary instructions. </p>
            </td>
           </tr>
            <tr onmouseout="PolicyIdling_stop()" onmouseover="PolicyIdling_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='PolicyIdling_blurry' class='hidden'><img src="images/www/policyidling.png"  width="160"></div>
               <div id='PolicyIdling_sharp'>
                 <a href="images/www/policyidling.png"><img src="images/www/policyidling.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function PolicyIdling_start() {
                   document.getElementById('PolicyIdling_blurry').style.display = 'inline';
                   document.getElementById('PolicyIdling_sharp').style.display = 'none';
                 }

                 function PolicyIdling_stop() {
                   document.getElementById('PolicyIdling_blurry').style.display = 'none';
                   document.getElementById('PolicyIdling_sharp').style.display = 'inline';
                 }
                 PolicyIdling_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Exploiting Policy Idling for Dexterous Manipulation</papertitle>
              <br>
              A. Chen, P. Brakel, A. Bronars, A. Xie, S. Huang, O. Groth, <strong>M. Bauza</strong>, et al.
              <br>
              <em>IROS </em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2508.15669" target="_blank">PDF</a>
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/PolicyIdling.bib" target="_blank">bibtex</a> / -->
              <!-- <a href="" target="_blank">website</a> -->
              <p></p>
              <p> We investigate how to leverage the detectability of idling behavior to inform exploration and policy improvement. Our approach, Pause-Induced Perturbations (PIP), applies perturbations at detected idling states, thus helping it to escape problematic basins of attraction.</p>
            </td>
           </tr>




            <tr bgcolor="#ffffd0" onmouseout="DemoStart_stop()" onmouseover="DemoStart_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='DemoStart_blurry' class='hidden'><img src="images/www/demostart.gif"  width="160"></div>
               <div id='DemoStart_sharp'>
                 <a href="images/www/demostart.png"><img src="images/www/demostart.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function DemoStart_start() {
                   document.getElementById('DemoStart_blurry').style.display = 'inline';
                   document.getElementById('DemoStart_sharp').style.display = 'none';
                 }

                 function DemoStart_stop() {
                   document.getElementById('DemoStart_blurry').style.display = 'none';
                   document.getElementById('DemoStart_sharp').style.display = 'inline';
                 }
                 DemoStart_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>DemoStart: Demonstration-led autocurriculum applied to sim-to-real with multi-fingered robots</papertitle>
              <br>
              <strong>M. Bauza</strong>, J. Chen, V. Dalibard, N. Gileadi, et al.
              <br>
              <em>ICRA 2025</em>
              <br>
              <a href="https://arxiv.org/pdf/2409.06613" target="_blank">PDF</a> /
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/DemoStart.bib" target="_blank">bibtex</a> / -->
              <a href="https://sites.google.com/view/demostart" target="_blank">website</a>
              <p></p>
              <p> DemoStart is an auto-curriculum reinforcement learning method capable of learning complex manipulation behaviors on an arm equipped with a three-fingered robotic hand, from only a sparse reward and a handful of demonstrations in simulation.</p>
            </td>
           </tr>

            <tr bgcolor="#ffffd0" onmouseout="Z_stop()" onmouseover="Z_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='Z_blurry' class='hidden'><img src="images/www/simPLE.webp"  width="160"></div>
               <div id='Z_sharp'>
                 <a href="images/www/simPLE.webp"><img src="images/www/simPLE_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function Z_start() {
                   document.getElementById('Z_blurry').style.display = 'inline';
                   document.getElementById('Z_sharp').style.display = 'none';
                 }

                 function Z_stop() {
                   document.getElementById('Z_blurry').style.display = 'none';
                   document.getElementById('Z_sharp').style.display = 'inline';
                 }
                 Z_stop()
               </script>
             </td>


             <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle> simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, T. Bronars, Y. Hou, I. Taylor, N. Chavan-Dafle, A. Rodriguez
              <br>
              <em> Science Robotics </em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2307.13133" target="_blank">PDF</a> /
              <!--<a href="https://youtu.be/OF8HbbGHnsM" target="_blank">video</a> / -->
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://mcube.mit.edu/research/simPLE.html" target="_blank"> website </a>
              <p></p>
              <p>We learn in simulation how to accurate pick-and-place objects with visuo-tactile perception. Our solution transfers to the real world and succefully handles diferent types of objects shapes without requiring prior experience.</p>
            </td>
           </tr>

           <tr onmouseout="LMPC_stop()" onmouseover="LMPC_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div id='LMPC_blurry' class='hidden'><img src="images/www/lmpc.gif"  width="160"></div>
              <div id='LMPC_sharp'>
                <a href="images/www/lmpc.png"><img src="images/www/lmpc.png"  width="160"></a>
              </div>
              <script type="text/javascript">
                function LMPC_start() {
                  document.getElementById('LMPC_blurry').style.display = 'inline';
                  document.getElementById('LMPC_sharp').style.display = 'none';
                }

                function LMPC_stop() {
                  document.getElementById('LMPC_blurry').style.display = 'none';
                  document.getElementById('LMPC_sharp').style.display = 'inline';
                }
                LMPC_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning to learn faster from human feedback with language model predictive control</papertitle>
              <br>
              Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, <strong>Maria Bauza</strong>, et al.
              <br>
              <em>RSS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2402.11450" target="_blank">PDF</a> /
              <!-- <a href="" target="_blank">video</a> / -->
              <!-- <a href="data/LMPC.bib" target="_blank">bibtex</a> / -->
              <a href="https://robot-teaching.github.io/" target="_blank">website</a>
              <p></p>
              <p> Language Model Predictive Control (LMPC) is a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments. LMPC accelerates fast robot adaptation via in-context learning. </p> 
            </td>
           </tr>


           <tr onmouseout="RoboCat_stop()" onmouseover="RoboCat_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='RoboCat_blurry' class='hidden'><img src="images/www/robocat.gif"  width="160"></div>
               <div id='RoboCat_sharp'>
                 <a hrewef="images/www/robocat.png"><img src="images/www/robocat.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function RoboCat_start() {
                   document.getElementById('RoboCat_blurry').style.display = 'inline';
                   document.getElementById('RoboCat_sharp').style.display = 'none';
                 }

                 function RoboCat_stop() {
                   document.getElementById('RoboCat_blurry').style.display = 'none';
                   document.getElementById('RoboCat_sharp').style.display = 'inline';
                 }
                 RoboCat_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation</papertitle>
              <br>
              K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. Lee, <strong>M. Bauza</strong>, et al.
              <br>
              <em>TMLR</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2306.11706" target="_blank">PDF</a> /
              <!--<a href="https://youtu.be/OF8HbbGHnsM" target="_blank">video</a> / -->
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/" target="_blank"> website </a>
              <p></p>
              <p>We introduce a self-improving AI agent for robotics, RoboCat, that learns to perform a variety of tasks across different arms, and then self-generates new training data to improve its technique.</p>

            </td>
           </tr>


            <tr onmouseout="X_stop()" onmouseover="X_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='X_blurry' class='hidden'><img src="images/www/fingerSLAM.webp"  width="160"></div>
               <div id='X_sharp'>
                 <a href="images/www/fingerSLAM.webp"><img src="images/www/fingerSLAM.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function X_start() {
                   document.getElementById('X_blurry').style.display = 'inline';
                   document.getElementById('X_sharp').style.display = 'none';
                 }

                 function X_stop() {
                   document.getElementById('X_blurry').style.display = 'none';
                   document.getElementById('X_sharp').style.display = 'inline';
                 }
                 X_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle> FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction from Visuo-tactile Feedback </papertitle>
              </a>
              <br>
              J. Zhao, <strong> M. Bauza</strong>, E. Adelson
              <br>
              <em>under review</em>, 2022 
              <br>
              <!--<a href="https://arxiv.org/pdf/2009.10623.pdf" target="_blank">PDF</a> -->
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p> We address the problem of using visuo-tactile feedback for 6-DoF localization and 3D reconstruction of unknown in-hand objects.</p>
            </td>
           </tr>
            <tr bgcolor="#ffffd0" onmouseout="Y_stop()" onmouseover="Y_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='Y_blurry' class='hidden'><img src="images/www/tac2pos.png"  width="160"></div>
               <div id='Y_sharp'>
                 <a href="images/www/tac2pos.png"><img src="images/www/tac2pos.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function Y_start() {
                   document.getElementById('Y_blurry').style.display = 'inline';
                   document.getElementById('Y_sharp').style.display = 'none';
                 }

                 function Y_stop() {
                   document.getElementById('Y_blurry').style.display = 'none';
                   document.getElementById('Y_sharp').style.display = 'inline';
                 }
                 Y_stop()
               </script>
             </td>


             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.05205.pdf" target="_blank" target="_blank">
                <papertitle> Tac2Pose: Tactile Object Pose Estimation from the First Touch" </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, T. Bronars, A. Rodriguez
              <br>
              <em>under review</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.11701.pdf" target="_blank">PDF</a>
              <!--<a href="https://youtu.be/OF8HbbGHnsM" target="_blank">video</a> / -->
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <!--<a href="https://mcube.mit.edu/research/tactile_loc_first_touch.html" target="_blank"> website </a>  -->
              <p></p>
              <p>We learn in simulation how to accurate localize objects with tactile. Our solution transfers to the real world, providing reliable pose distributions from the first touch.</p>
              <p>Our technology is used by <a href="https://www.magna.com/">Magna</a> and <a href="https://global.abb/group/">ABB</a> and MERL. Our tactile sensor is <a href="https://arxiv.org/abs/2103.12269" target="_blank">  Gelslim</a>. </p>
            </td>
           </tr>

           <tr onmouseout="D_stop()" onmouseover="D_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='D_blurry' class='hidden'><img src="images/www/tailoring.png"  width="160"></div>
               <div id='D_sharp'>
                 <a href="images/www/tailoring.png"><img src="images/www/tailoring.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function D_start() {
                   document.getElementById('D_blurry').style.display = 'inline';
                   document.getElementById('D_sharp').style.display = 'none';
                 }

                 function D_stop() {
                   document.getElementById('D_blurry').style.display = 'none';
                   document.getElementById('D_sharp').style.display = 'inline';
                 }
                 D_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2009.10623.pdf" target="_blank">
                <papertitle> Tailoring: Encoding Inductive Biases by Optimizing Unsupervised Objectives at Prediction Time </papertitle>
              </a>
              <br>
              F. Alet, K. Kawaguchi, <strong> M. Bauza</strong>, N. Kuru, T. Lozano-Perez, L. Kaelbling
              <br>
              <em>NeurIPS</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2009.10623.pdf" target="_blank">PDF</a> 
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p>We optimize unsupervised losses for the current input. By optimizing where we act, we bypass generalization gaps and can impose a wide variety of inductive biases.</p>
            </td>
           </tr>


           <tr onmouseout="C_stop()" onmouseover="C_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='C_blurry' class='hidden'><img src="images/www/icra21.webp"  width="160"></div>
               <div id='C_sharp'>
                 <a href="images/www/icra21.webp"><img src="images/www/icra21_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function C_start() {
                   document.getElementById('C_blurry').style.display = 'inline';
                   document.getElementById('C_sharp').style.display = 'none';
                 }

                 function C_stop() {
                   document.getElementById('C_blurry').style.display = 'none';
                   document.getElementById('C_sharp').style.display = 'inline';
                 }
                 C_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.07044.pdf" target="_blank">
                <papertitle> Real-time  shape  and  pose  estimation  from  planar  pushing using  implicit  surfaces</papertitle>
              </a>
              <br>
              S. Suresh,<strong> M.Bauza</strong>, A. Rodriguez, J. Mangelson, M. Kaess
              <br>
              <em>ICRA</em>, 2021  &nbsp <font color="green"><strong>(Best Paper Finalist on the ICRA21 Service Robotics Award)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2011.07044.pdf" target="_blank">PDF</a> /
              <a href="https://youtu.be/77VnwArHOhk" target="_blank">video</a> /
              <a href="https://github.com/suddhu/gpis-touch-public">code</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="http://www.cs.cmu.edu/~sudhars1/tactile-slam/" target="_blank"> website </a>
              <p></p>
              <p>In real-time, we infer from planar pushes both the shape and pose of an object.</p>
            </td>
           </tr>
           <tr bgcolor="#ffffd0" onmouseout="A_stop()" onmouseover="A_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='A_blurry' class='hidden'><img src="images/www/RSS20.webp"  width="160"></div>
               <div id='A_sharp'>
                 <a href="images/www/RSS20.webp"><img src="images/www/RSS20_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function A_start() {
                   document.getElementById('A_blurry').style.display = 'inline';
                   document.getElementById('A_sharp').style.display = 'none';
                 }

                 function A_stop() {
                   document.getElementById('A_blurry').style.display = 'none';
                   document.getElementById('A_sharp').style.display = 'inline';
                 }
                 A_stop()
               </script>
             </td>


             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.05205.pdf" target="_blank" target="_blank">
                <papertitle> Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, E. Valls, B. Lim, T. Sechopoulos
              <br>
              <em>CORL</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2012.05205.pdf" target="_blank">PDF</a> /
              <a href="https://youtu.be/OF8HbbGHnsM" target="_blank">video</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://mcube.mit.edu/research/tactile_loc_first_touch.html" target="_blank"> website </a>
              <p></p>
            </td>
           </tr>
                  
          
           <tr onmouseout="E_stop()" onmouseover="E_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='E_blurry' class='hidden'><img src="images/www/icra20.webp"  width="160"></div>
               <div id='E_sharp'>
                 <a href="images/www/icra20.webp"><img src="images/www/icra20_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function E_start() {
                   document.getElementById('E_blurry').style.display = 'inline';
                   document.getElementById('E_sharp').style.display = 'none';
                 }

                 function E_stop() {
                   document.getElementById('E_blurry').style.display = 'none';
                   document.getElementById('E_sharp').style.display = 'inline';
                 }
                 E_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1911.03112.pdf" target="_blank">
                <papertitle> Accurate Vision-based Manipulation through Contact Reasoning </papertitle>
              </a>
              <br>
              A. Kloss,<strong> M. Bauza</strong>, J. Wu, J. Tenenbaum, A. Rodriguez, J. Bohg
              <br>
              <em>ICRA</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1911.03112.pdf" target="_blank">PDF</a> /
              <a href="https://youtu.be/YLnXLHWTA60" target="_blank">video</a>
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <!-- <p>Accurate vision-based manipulation through contact reasoning.</p> -->
            </td>
           </tr>


           <!-- <tr bgcolor="#ffffd0" onmouseout="B_stop()" onmouseover="B_start()"> -->
           <!--            <tr onmouseout="B_stop()" onmouseover="B_start()"> -->
           <tr bgcolor="#ffffd0" onmouseout="B_stop()" onmouseover="B_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='B_blurry' class='hidden'><img src="images/www/tactile_loc.webp"  width="160"></div>
               <div id='B_sharp'>
                 <a href="images/www/tactile_loc.webp"><img src="images/www/tactile_loc_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function B_start() {
                   document.getElementById('B_blurry').style.display = 'inline';
                   document.getElementById('B_sharp').style.display = 'none';
                 }

                 function B_stop() {
                   document.getElementById('B_blurry').style.display = 'none';
                   document.getElementById('B_sharp').style.display = 'inline';
                 }
                 B_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1904.10944.pdf" target="_blank">
                <papertitle> Tactile Mapping and Localization from High-Resolution Tactile Imprints </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, O. Canal, A. Rodriguez
              <br>
              <em>ICRA</em>, 2019 
              <br>
              <a href="https://arxiv.org/pdf/1904.10944.pdf" target="_blank">PDF</a> /
              <a href="https://youtu.be/uMkspjmDbqs" target="_blank">video</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://mcube.mit.edu/research/tactile_localization.html" target="_blank"> website </a>
              <p></p>
              <p>Shape reconstruction and object localization using the vision-based tactile sensor GelSlim.</p>
            </td>
           </tr>
           
           <tr onmouseout="F_stop()" onmouseover="F_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='F_blurry' class='hidden'><img src="images/www/corl.gif"  width="160"></div>
               <div id='F_sharp'>
                 <a href="images/www/corl.gif"><img src="images/www/corl_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function F_start() {
                   document.getElementById('F_blurry').style.display = 'inline';
                   document.getElementById('F_sharp').style.display = 'none';
                 }

                 function F_stop() {
                   document.getElementById('F_blurry').style.display = 'none';
                   document.getElementById('F_sharp').style.display = 'inline';
                 }
                 F_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1911.05071.pdf" target="_blank">
                <papertitle> Experience-Embedded Visual Foresight </papertitle>
              </a>
              <br>
              Y. Lin,<strong> M. Bauza</strong>, P. Isola
              <br>
              <em>CORL</em>, 2019  
              <br>
              <a href="https://arxiv.org/pdf/1911.05071.pdf" target="_blank">PDF</a> /
              <a href="https://github.com/yenchenlin/evf-public">code</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://yenchenlin.me/evf/" target="_blank"> website </a>
              <p></p>
              <p>Learning to encode new objects to generate physically plausible video predictions.</p>
            </td>
           </tr>
           <tr onmouseout="G_stop()" onmouseover="G_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='G_blurry' class='hidden'><img src="images/www/regrasp.webp"  width="160"></div>
               <div id='G_sharp'>
                 <a href="images/www/regrasp.webp"><img src="images/www/regrasp_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function G_start() {
                   document.getElementById('G_blurry').style.display = 'inline';
                   document.getElementById('G_sharp').style.display = 'none';
                 }

                 function G_stop() {
                   document.getElementById('G_blurry').style.display = 'none';
                   document.getElementById('G_sharp').style.display = 'inline';
                 }
                 G_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1803.01940.pdf" target="_blank">
                <papertitle> Tactile Regrasp: Grasp Adjustments via Simulated Tactile Transformations </papertitle>
              </a>
              <br>
              <strong>M. Bauza*</strong>, F. Hogan* , O. Canals, A. Rodriguez
              <br>
              <em>IROS</em>, 2018 &nbsp <font color="green"><strong> (Best Poster Award at ICRA 2018 workshop)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1803.01940.pdf" target="_blank">PDF</a> /
              <a href="https://www.youtube.com/watch?v=gjn7DmfpwDk" target="_blank">video</a>
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p>Regrasping using a high-resolution tactile sensor to improve grasp stability. </p>
            </td>
           </tr>
           <tr onmouseout="H_stop()" onmouseover="H_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='H_blurry' class='hidden'><img src="images/www/GENs.webp"  width="160"></div>
               <div id='H_sharp'>
                 <a href="images/www/GENs.webp"><img src="images/www/GENs_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function H_start() {
                   document.getElementById('H_blurry').style.display = 'inline';
                   document.getElementById('H_sharp').style.display = 'none';
                 }

                 function H_stop() {
                   document.getElementById('H_blurry').style.display = 'none';
                   document.getElementById('H_sharp').style.display = 'inline';
                 }
                 H_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1904.09019.pdf" target="_blank">
                <papertitle> Graph Element Networks: adaptive, structured computation and memory </papertitle>
              </a>
              <br>
              F. Alet, A. Jeewajee,<strong> M. Bauza</strong>, A. Rodriguez, T. Lozano-Perez, L. Kaelbling
              <br>
              <em>ICML</em>, 2019  &nbsp <font color="green"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1904.09019.pdf" target="_blank">PDF</a> /
              <a href="https://youtu.be/wp9CjkOQm48" target="_blank">video</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://github.com/FerranAlet/graph_element_networks" target="_blank"> website </a>
              <p></p>
              <p>We learn to map functions to functions by combining graph networks and attention to build computational meshes and show this new framework can solve very diverse problems.</p>
            </td>
           </tr>
           <tr onmouseout="H2_stop()" onmouseover="H2_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='H2_blurry' class='hidden'><img src="images/www/object_omni.webp"  width="160"></div>
               <div id='H2_sharp'>
                 <a href="images/www/object_omni.webp"><img src="images/www/object_omni_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function H2_start() {
                   document.getElementById('H2_blurry').style.display = 'inline';
                   document.getElementById('H2_sharp').style.display = 'none';
                 }

                 function H2_stop() {
                   document.getElementById('H2_blurry').style.display = 'none';
                   document.getElementById('H2_sharp').style.display = 'inline';
                 }
                 H2_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1910.00618.pdf" target="_blank">
                <papertitle> Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, F. Alet, Y. Lin, T. Lozano-Perez, L. Kaelbling, P. Isola, A. Rodriguez
              <br>
              <em>IROS</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1910.00618.pdf" target="_blank">PDF</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="http://mit.edu/mcube/omnipush-dataset/" target="_blank"> website </a>
              <p></p>
              <p>We present a large high-quality dataset on planar pushing that includes RGB-D video and extense object variability.</p>
              
            </td>
           </tr>           
           <tr onmouseout="I_stop()" onmouseover="I_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='I_blurry' class='hidden'><img src="images/www/data_control.webp"  width="160"></div>
               <div id='I_sharp'>
                 <a href="images/www/data_control.webp"><img src="images/www/data_control_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function I_start() {
                   document.getElementById('I_blurry').style.display = 'inline';
                   document.getElementById('I_sharp').style.display = 'none';
                 }

                 function I_stop() {
                   document.getElementById('I_blurry').style.display = 'none';
                   document.getElementById('I_sharp').style.display = 'inline';
                 }
                 I_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1807.09904.pdf" target="_blank">
                <papertitle> A Data-Efficient Approach to Precise and Controlled Pushing </papertitle>
              </a>
              <br>
              <strong>M. Bauza*</strong>, F. Hogan* , A. Rodriguez
              <br>
              <em>CORL</em>, 2018
              <br>
              <a href="https://arxiv.org/pdf/1807.09904.pdf" target="_blank">PDF</a> /
              <a href="https://www.youtube.com/watch?v=Z45O480pij0" target="_blank">video</a>
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p>We explore the data-complexity required for controlling, rather than modeling, planar pushing.</p>
              
            </td>
           </tr>
           <tr onmouseout="J_stop()" onmouseover="J_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='J_blurry' class='hidden'><img src="images/www/2circles.webp"  width="160"></div>
               <div id='J_sharp'>
                 <a href="images/www/2circles.webp"><img src="images/www/2circles_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function J_start() {
                   document.getElementById('J_blurry').style.display = 'inline';
                   document.getElementById('J_sharp').style.display = 'none';
                 }

                 function J_stop() {
                   document.getElementById('J_blurry').style.display = 'none';
                   document.getElementById('J_sharp').style.display = 'inline';
                 }
                 J_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1904.06580.pdf" target="_blank">
                <papertitle> Combining Physical Simulators and Object-Based Networks for Control </papertitle>
              </a>
              <br>
              A. Ajay,<strong> M. Bauza</strong>, J. Wu, N. Fazeli, J. Tenenbaum, A. Rodriguez
              <br>
              <em>ICRA</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1904.06580.pdf" target="_blank">PDF</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="http://sain.csail.mit.edu/" target="_blank"> website </a>
              <p></p>
              <p>We propose a hybrid dynamics model, simulator-augmented interaction networks (SAIN), combining a physics engine with an object-based neural network for dynamics modeling.</p>
              
            </td>
           </tr>
           <tr onmouseout="K_stop()" onmouseover="K_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='K_blurry' class='hidden'><img src="images/www/stochastic_nn.png"  width="160"></div>
               <div id='K_sharp'>
                 <a href="images/www/stochastic_nn.png"><img src="images/www/stochastic_nn.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function K_start() {
                   document.getElementById('K_blurry').style.display = 'inline';
                   document.getElementById('K_sharp').style.display = 'none';
                 }

                 function K_stop() {
                   document.getElementById('K_blurry').style.display = 'none';
                   document.getElementById('K_sharp').style.display = 'inline';
                 }
                 K_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1808.03246.pdf" target="_blank">
                <papertitle> Augmenting Simulators with Stochastic Networks </papertitle>
              </a>
              <br>
              A. Ajay, J. Wu, N. Fazeli,<strong> M. Bauza</strong>, L. Kaelbling, J. Tenenbaum, A. Rodriguez
              <br>
              <em>IROS</em>, 2018  &nbsp <font color="green"><strong>(Best Paper Award on Cognitive Robotics)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1808.03246.pdf" target="_blank">PDF</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="http://physplus.csail.mit.edu/" target="_blank"> website </a>
              <p></p>
              <p>We augment an analytical rigid-body simulator with a neural network that learns to model uncertainty as residuals. Best Paper Award on Cognitive Robotics at IROS 2018.</p>
              
            </td>
           </tr>
           <tr onmouseout="L_stop()" onmouseover="L_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='L_blurry' class='hidden'><img src="images/www/arc.webp"  width="160"></div>
               <div id='L_sharp'>
                 <a href="images/www/arc.webp"><img src="images/www/arc_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function L_start() {
                   document.getElementById('L_blurry').style.display = 'inline';
                   document.getElementById('L_sharp').style.display = 'none';
                 }

                 function L_stop() {
                   document.getElementById('L_blurry').style.display = 'none';
                   document.getElementById('L_sharp').style.display = 'inline';
                 }
                 L_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://vision.princeton.edu/projects/2017/arc/paper.pdf" target="_blank">
                <papertitle> Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching </papertitle>
              </a>
              <br>
              A Zeng, S Song, K. Yu, E. Donlon, F. Hogan,<strong> M. Bauza</strong>, et. al.
              <br>
              <em>ICRA</em>, 2018  &nbsp <font color="green"><strong>(Best Systems Paper Award by Amazon Robotics)</strong></font>
              <br>
              <a href="https://vision.princeton.edu/projects/2017/arc/paper.pdf" target="_blank">PDF</a> /
              <a href="https://www.youtube.com/watch?v=6fG7zwGfIkI" target="_blank">video</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="https://arc2017.mit.edu/" target="_blank"> website </a>
              <p></p>
              <p>With the MIT-Princeton team we developed a robust robotic system for bin picking.</p>
              
            </td>
           </tr>
           <tr onmouseout="M_stop()" onmouseover="M_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='M_blurry' class='hidden'><img src="images/www/GP-SUM.webp"  width="160"></div>
               <div id='M_sharp'>
                 <a href="images/www/GP-SUM.webp"><img src="images/www/GP-SUM_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function M_start() {
                   document.getElementById('M_blurry').style.display = 'inline';
                   document.getElementById('M_sharp').style.display = 'none';
                 }

                 function M_stop() {
                   document.getElementById('M_blurry').style.display = 'none';
                   document.getElementById('M_sharp').style.display = 'inline';
                 }
                 M_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1709.08120.pdf" target="_blank">
                <papertitle> GP-SUM. Gaussian Processes Filtering of non-Gaussian Beliefs </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, A. Rodriguez
              <br>
              <em>WAFR</em>, 2018
              <br>
              <a href="https://arxiv.org/pdf/1709.08120.pdf" target="_blank">PDF</a>
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p>We developed the algorithm GP-SUM: a GP-Bayes filter that propagates in time non-Gaussian beliefs.</p>
              
            </td>
           </tr>
           <tr onmouseout="N_stop()" onmouseover="N_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='N_blurry' class='hidden'><img src="images/www/data-driven.webp"  width="160"></div>
               <div id='N_sharp'>
                 <a href="images/www/data-driven.webp"><img src="images/www/data-driven_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function N_start() {
                   document.getElementById('N_blurry').style.display = 'inline';
                   document.getElementById('N_sharp').style.display = 'none';
                 }

                 function N_stop() {
                   document.getElementById('N_blurry').style.display = 'none';
                   document.getElementById('N_sharp').style.display = 'inline';
                 }
                 N_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1704.03033.pdf" target="_blank">
                <papertitle> A Probabilistic Data-Driven Model for Planar Pushing </papertitle>
              </a>
              <br>
              <strong>M. Bauza</strong>, A. Rodriguez
              <br>
              <em>ICRA</em>, 2017
              <br>
              <a href="https://arxiv.org/pdf/1704.03033.pdf" target="_blank">PDF</a>
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> -->
              <p></p>
              <p>Characterizing the uncertainty of different pushes allows better action selection.</p>
              
            </td>
           </tr>
                                                                             <tr onmouseout="O_stop()" onmouseover="O_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <div id='O_blurry' class='hidden'><img src="images/www/push.webp"  width="160"></div>
               <div id='O_sharp'>
                 <a href="images/www/push.webp"><img src="images/www/push_S.png"  width="160"></a>
               </div>
               <script type="text/javascript">
                 function O_start() {
                   document.getElementById('O_blurry').style.display = 'inline';
                   document.getElementById('O_sharp').style.display = 'none';
                 }

                 function O_stop() {
                   document.getElementById('O_blurry').style.display = 'none';
                   document.getElementById('O_sharp').style.display = 'inline';
                 }
                 O_stop()
               </script>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1604.04038.pdf" target="_blank">
                <papertitle> More than a Million Ways to Be Pushed. A High-Fidelity Experimental Data Set of Planar Pushing </papertitle>
              </a>
              <br>
              K. Yu,<strong> M. Bauza</strong>, N. Fazeli, and A. Rodriguez <!--   <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a> -->
              <br>
              <em>IROS</em>, 2016  &nbsp <font color="green"><strong>(Best Paper Finalist at IROS)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1604.04038.pdf" target="_blank">PDF</a> /
              <a href="https://www.youtube.com/watch?v=tuXCHFnc7DY" target="_blank">video</a> /
              <!-- <a href="data/Anderson2016.bib" target="_blank">bibtex</a> / -->
              <a href="http://mcube.mit.edu/push-dataset/index.html" target="_blank"> website </a>
              <p></p>
              <p>More than a million datapoints collected on real pushing experiments.</p>
              
            </td>
           </tr>
        </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Older News</heading>
                <p>
                <strong> July 2022 </strong> Invited talk at the RSS 2022 workshop on The Science of Bumping Into Things.
                </p><p>
                <strong> May 2022 </strong> Co-organized the workshop at ICRA 2022 on Bi-manual Manipulation: Addressing Real-world Challenges.
                </p><p> 
                <strong> March 2022 </strong> Invited talks at <a href="https://memento.epfl.ch/event/ic-colloquium-learning-to-interact-with-the-worl-2/" target="_blank">EPFL</a>, 
                            <a href="https://engineering.princeton.edu/events/learning-interact-world-when-generality-meets-precision" target="_blank">Princeton University</a>, 
                            <a href="https://www.cs.cmu.edu/calendar/158366868" target="_blank">CMU</a>, and 
                            <a href="https://events.seas.upenn.edu/event/ese-spring-seminar-learning-to-interact-with-the-world-when-generality-meets-precision/?utm_source=rss&utm_medium=rss&utm_campaign=ese-spring-seminar-learning-to-interact-with-the-world-when-generality-meets-precision" target="_blank">University of Pennsylvania</a>.
                </p><p> 
                <strong>February 2022 </strong> Invited talks at 
                                <a href="https://www.physics.columbia.edu/events/me-seminar-dr-maria-bauza" target="_blank">Columbia University</a>, 
                                <a href="https://www.youtube.com/watch?v=fZvB2nRbGFM" target="_blank">the Autonomy Talks at ETH Zurich</a>, and 
                                <a href="https://events.cornell.edu/event/seminar_cornell_tech_maria_bauza_villalonga" target="_blank">Cornell Tech</a>.
                </p><p> 
                <strong>December 2021 </strong> Invited talk at the Washington University robotics colloquium.
                </p><p> 
                <strong>November 2021 </strong> Invited talks at Stanford and the CMU Manipulation discussion grup.
                </p><p> 
                <strong>October 2021 </strong>Invited talk at Cornell Robotic Seminar and selected to attend the Rising Stars in EECS.
                </p><p>
                <strong>July 2021</strong> Attended the 2021 RSS Pioneers Workshop.
                </p><p>
                <strong>May 2021</strong> Best Paper Finalist Award on Service Robotics at ICRA 2021
                </p><p>
                <strong>March 2021</strong> Invited talk at University of Toronto, AI in Robotics Seminar.
                </p><p>
                <strong>October 2020</strong> Invited talk at University of Pennsylvania, Grasp Seminar.
                </p><p>
                <strong>May 2020 </strong>Co-organizing workshop at ICRA 2020 on Uncertainty in Contact-Rich Interactions.
                </p><p>
                <strong>November 2019 </strong> Selected to attend the Global Young Scientists Summit. Awarded to only 5 PhDs from all MIT departments.
                </p><p>
                <strong>October 2019 </strong> Rising Stars in Mechanical Engineering. Awarded to 30 graduate and postdoctoral women.
                </p><p>
                <strong>Januray 2019 </strong> Awarded the Facebook Emerging Scholar Award. 21 awardees out of more than 900 applications.
                </p><p>
                <strong>December 2018 </strong> Awarded the NVIDIA Graduate Fellowship. Given to 10 PhD students from more than 230 applications.
                </p>
            </td>
          </tr>
        </tbody></table>
        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </tbody></table>
        -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks for sharing <a href="https://jonbarron.info/"  target="_blank"> this amazing web design</a>.
              </p>
            </td>
          </tr>
        </tbody></table>        
      </td>
    </tr>
  </table>
</body>

</html>

